# The Measure for Weak Emergence: Taking Data Compression Algorithms Philosophically Seriously

**[A. Theodore Izmaylov](https://theodoreizmaylov.com/), 2024.**

*Keywords: weak emergence, reductionism, Conway’s Game of Life, Kolmogorov complexity, algorithmic complexity, data compression, explanatory incompressibility*

Bedau's (2008) weak emergence provides a philosophically viable alternative to reductionism while staying scientifically relevant. Its two definitions are based on explanatory incompressibility and underivability except by simulation and allow to claim a relative autonomy of higher-level laws and entities while staying metaphysically innocent by avoiding the need to declare their ontological novelty and discontinuity with the lower-level. Since no apriori grounds are given for the dichotomy between emergent and reducible, it permits or even requires weak emergence to come in degrees. Hovda and Berenstain suggested two opposing approaches to such quantification of weak emergence.

Hovda's (2008) approach follows Bedau in using cellular automata, Conway's Game of Life (GOL) in particular, to keep philosophical arguments precise and practically relevant. The suggested quantification technique starts with a very narrow definition of simulation as a computation necessarily based on the original unmodified and unamended micro-level rules of GOL and shows how the amount of such simulation required to derive macrostates from microstates in various cases can be measured and capped. Hovda calls it "s-derivability" and readily admits that it is required first to make an arbitrary choice of the derivation system to which such measure is relative. This approach offers formulas allowing precise and practically computable figures for the degrees of weak emergence in each case.

However, as I argue, it comes at the expense of rendering weak emergence under such simulation specification philosophically and scientifically irrelevant. The former is because defining weak emergence through simulation and using GOL as a model were technical means to philosophical ends which are no longer met if the definition of simulation is, in turn, narrowed to and linked with the step-by-step execution of the precisely specified micro-rules of GOL – it becomes just too specific for philosophers to care. The latter is because most real-world scientific simulations do not use the precise, logical structures of the underlying laws and theories but resort to more convenient and computationally feasible equivalents, heuristics, approximations and optimisations. In this sense, even the actual software for simulating GOL will not satisfy Hovda's criteria of using any amount of simulation because it does not use the exact micro-laws of GOL.

Berenstain's (2022) approach strives to quantify and claims to strengthen the weak emergence. She critiques Hovda's solution for its measure of the degree of emergence being relative to an arbitrary choice of the derivation system and, therefore, as not strong enough for being philosophically relevant. Berenstain's solution is to use Kolmogorov's algorithmic complexity as an absolute measure of compressibility of a particular emergent state's description and, therefore, as a degree of emergence. Kolmogorov's algorithmic complexity gives a precise asymptotic measure independent of a system of derivation, a particular algorithm or a turning machine used for compression because it, by definition, accounts for any and all of them.

Unfortunately, as I will argue, this approach yet again turns out to be both scientifically and philosophically irrelevant, although in a different way. It is practically irrelevant to science because Kolmogorov's algorithmic complexity is uncomputable by definition, and the main point of suggesting a measure for the degree of emergence was the ability to measure it at the outset of scientific inquiry to justify using reductive lower-level or holistic higher-level approaches, methods and theories. 

However, under this definition, it is, at best, possible to estimate the degree of emergence only after the scientific study is complete description is given. It is also irrelevant to the philosophy of science because despite the precise and absolute value of Kolmogorov's complexity mathematically existing for every emergent state, it is, by definition, not empirically accessible via any scientifically possible means.

I suggest an alternative approach which takes data compression algorithms philosophically seriously and offers a theoretically and practically relevant model of scientific inquiry into emergent phenomena. It primarily focuses on dictionary methods of data compression as a metaphor for or even a model of scientific description and explanation and gives a measure of a degree of weak emergence as a result. The core idea of the approach is that relativity of the degree of emergence is intrinsic to the scientific process and can be viewed as relativity of compressibility to a specific dictionary used in dictionary methods of data compression.

For we do not care about the amounts of strictly defined simulation needed to derive a macrostate, nor do we care about the elusive theoretical degree of compressibility of a random piece of data. What we do care about is applying our scientific dictionary of basic entities to describe the empirical data at hand and establish lawful patterns in it which explain more while using less (i.e. compress) and then expanding and sometimes modifying this dictionary to describe data and patterns better. This is strikingly similar to data compression with dictionary methods, and I will elaborate on this symmetry in the context of the reductionism debate in the philosophy of science in general and in relation to weak emergence as a solution and its quantification in particular.

Ironically, on closer inspection, data compression algorithms are what may actually ground Berenstain's abstract approach because popular dictionary methods of data compression are widely used to estimate Kolmogorov's complexity in practice. It may also elevate Hovda's approach by quantifying various ways of compressing the time and space needed for GOL-like simulations instead of fixating on the literal logical structure of GOL. In general, this is faithful to and builds on Bedau's approach of gaining intuitions and developing philosophical arguments by seriously examining theoretical and practical resources from computer science. In particular, it may help to improve the often mistaken or fragmented reception of weak emergence and appreciation of its implication for reductionism debate in the philosophy of science.

### Selected references

Bedau, M. A. (2008). Is Weak Emergence Just in the Mind? Minds and Machines, 18(4), 443–459. https://doi.org/10.1007/s11023-008-9122-6

Berenstain, N. (2022). Strengthening Weak Emergence. Erkenntnis, 87(5), 2457–2474. https://doi.org/10.1007/s10670-020-00312-6

Hovda, P. (2008). Quantifying Weak Emergence. Minds and Machines, 18(4), 461–473. https://doi.org/10.1007/s11023-008-9123-5

### Accepted and scheduled for presentation at
The International Association of Computing and Philosophy IACAP 2024 Conference in Eugene, Oregon, USA, July 9, 2024. (Not presented due to visa issues)